# KayÄ±p FonksiyonlarÄ± (Loss Functions)

Loss fonksiyonlarÄ±, derin Ã¶ÄŸrenme modellerinin baÅŸarÄ±sÄ±nÄ± deÄŸerlendirmede ve optimize etmede kullanÄ±lan temel matematik araÃ§larÄ±dÄ±r. Bu fonksiyonlar, modelin tahminleri ile gerÃ§ek deÄŸerler arasÄ±ndaki sapmanÄ±n nicel bir Ã¶lÃ§Ã¼sÃ¼nÃ¼ saÄŸlar ve modelin eÄŸitimi sÄ±rasÄ±nda bu sapmanÄ±n minimize edilmesine rehberlik eder.

## 1. Categorical Cross-Entropy
**Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma** problemlerinin standardÄ± olan bu fonksiyon, gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma, duygu analizi, nesne tanÄ±ma gibi birden fazla sÄ±nÄ±f iÃ§eren problemlerde kullanÄ±lÄ±r. Her bir sÄ±nÄ±f iÃ§in olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± Ã¼reterek, doÄŸru sÄ±nÄ±fÄ±n olasÄ±lÄ±ÄŸÄ±nÄ± maksimize etmeye Ã§alÄ±ÅŸÄ±r.

```python
import tensorflow as tf

loss = tf.keras.losses.CategoricalCrossentropy()
```

### ğŸ” Ã–zellikleri:
- One-hot encoded etiketlerle Ã§alÄ±ÅŸÄ±r (Ã¶rn: [0,1,0] ÅŸeklinde)
- Softmax aktivasyon fonksiyonu ile birlikte optimum performans gÃ¶sterir
- SÄ±nÄ±flar arasÄ± dengeli bir Ã¶ÄŸrenme saÄŸlar
- Gradyan patlamasÄ±/sÃ¶nmesi problemlerine karÅŸÄ± direnÃ§lidir

## 2. Binary Cross-Entropy
**Ä°kili sÄ±nÄ±flandÄ±rma** problemlerinin vazgeÃ§ilmez loss fonksiyonudur. Ã–zellikle gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma, spam tespiti, hastalÄ±k teÅŸhisi gibi evet/hayÄ±r kararlarÄ±nÄ±n verildiÄŸi durumlarda kullanÄ±lÄ±r. Matematiksel olarak, tahmin edilen olasÄ±lÄ±klarÄ±n log-likelihood deÄŸerini maksimize etmeye Ã§alÄ±ÅŸÄ±r.

```python
def binary_cross_entropy(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
```

### ğŸ” Ã–zellikleri:
- Sigmoid aktivasyon fonksiyonu ile birlikte kullanÄ±lÄ±r
- 0'a yakÄ±n yanlÄ±ÅŸ tahminleri ve 1'e yakÄ±n doÄŸru tahminleri teÅŸvik eder
- Gradyan deÄŸerleri otomatik olarak Ã¶lÃ§eklenir
- Dengesiz veri setlerinde class_weight parametresi ile desteklenebilir

## 3. Mean Squared Error (MSE)
**Regresyon** problemlerinin temel taÅŸÄ± olan MSE, tahmin edilen deÄŸerler ile gerÃ§ek deÄŸerler arasÄ±ndaki farkÄ±n karesinin ortalamasÄ±nÄ± hesaplar. Bu fonksiyon, Ã¶zellikle ev fiyatÄ± tahmini, hava sÄ±caklÄ±ÄŸÄ± Ã¶ngÃ¶rÃ¼sÃ¼ gibi sÃ¼rekli deÄŸer tahminlerinde tercih edilir.

```python
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
```

### ğŸ” Ã–zellikleri:
- Her zaman pozitif deÄŸer Ã¼retir, Ã§Ã¼nkÃ¼ farklarÄ±n karesi alÄ±nÄ±r
- BÃ¼yÃ¼k hatalar karesel olarak cezalandÄ±rÄ±lÄ±r, bu nedenle aykÄ±rÄ± deÄŸerlere karÅŸÄ± hassastÄ±r
- TÃ¼revi kolay hesaplanabilir, bu da gradyan iniÅŸi iÃ§in avantaj saÄŸlar
- Hedef deÄŸiÅŸken normal daÄŸÄ±lÄ±ma sahip olduÄŸunda optimal sonuÃ§ verir

## 4. Mean Absolute Error (MAE)
MSE'ye alternatif olan MAE, tahminler ile gerÃ§ek deÄŸerler arasÄ±ndaki mutlak farklarÄ±n ortalamasÄ±nÄ± alÄ±r. AykÄ±rÄ± deÄŸerlere karÅŸÄ± MSE'den daha az hassastÄ±r.

```python
def mae_loss(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))
```

### ğŸ” Ã–zellikleri:
- Mutlak deÄŸer kullanÄ±ldÄ±ÄŸÄ± iÃ§in aykÄ±rÄ± deÄŸerlere karÅŸÄ± MSE'den daha dayanÄ±klÄ±dÄ±r
- Her zaman pozitif deÄŸer Ã¼retir
- Lineer Ã¶lÃ§ekleme saÄŸlar, bÃ¼yÃ¼k hatalarÄ± MSE kadar ÅŸiddetli cezalandÄ±rmaz
- Medyan tahminlerinde optimal sonuÃ§ verir

## 5. Huber Loss
MSE ve MAE'nin hibrit bir versiyonudur. KÃ¼Ã§Ã¼k hatalar iÃ§in MSE gibi, bÃ¼yÃ¼k hatalar iÃ§in MAE gibi davranÄ±r. AykÄ±rÄ± deÄŸerlere karÅŸÄ± MSE'den daha direnÃ§lidir.

```python
def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    is_small_error = np.abs(error) <= delta
    squared_loss = 0.5 * np.square(error)
    linear_loss = delta * np.abs(error) - 0.5 * np.square(delta)
    return np.mean(np.where(is_small_error, squared_loss, linear_loss))
```

### ğŸ” Ã–zellikleri:
- Delta parametresi ile MSE ve MAE arasÄ±nda geÃ§iÅŸ yapÄ±labilir
- AykÄ±rÄ± deÄŸerlere karÅŸÄ± MSE'den daha dayanÄ±klÄ±dÄ±r
- KÃ¼Ã§Ã¼k hatalar iÃ§in karesel, bÃ¼yÃ¼k hatalar iÃ§in lineer ceza uygular
- Regresyon problemlerinde dengeli bir seÃ§enek sunar

## 6. Focal Loss
Dengesiz veri setlerinde **sÄ±nÄ±flandÄ±rma** problemleri iÃ§in tasarlanmÄ±ÅŸ Ã¶zel bir kayÄ±p fonksiyonudur. Zor Ã¶rneklere daha fazla aÄŸÄ±rlÄ±k vererek, modelin sÄ±k gÃ¶rÃ¼len Ã¶rnekler yerine nadir gÃ¶rÃ¼len Ã¶rneklere odaklanmasÄ±nÄ± saÄŸlar.

```python
def focal_loss(y_true, y_pred, gamma=2.0):
    ce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
    pt = tf.exp(-ce)
    return tf.mean((1 - pt) ** gamma * ce)
```

### ğŸ” Ã–zellikleri:
- Dengesiz veri setlerinde performansÄ± artÄ±rÄ±r
- Gamma parametresi ile kolay/zor Ã¶rnekler arasÄ±ndaki denge ayarlanabilir
- Binary Cross-Entropy'nin geliÅŸtirilmiÅŸ versiyonudur
- Nesne tespiti gibi sÄ±nÄ±f dengesizliÄŸi olan problemlerde yaygÄ±n kullanÄ±lÄ±r

## 7. Kullback-Leibler Divergence (KL Divergence)
Ä°ki olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± arasÄ±ndaki farkÄ± Ã¶lÃ§en bir kayÄ±p fonksiyonudur. Ã–zellikle Variational Autoencoder (VAE) ve probabilistik modellerde kullanÄ±lÄ±r.

```python
def kl_divergence(p, q):
    return tf.reduce_sum(p * tf.math.log(p / q))
```

### ğŸ” Ã–zellikleri:
- DaÄŸÄ±lÄ±mlar arasÄ±ndaki benzerliÄŸi Ã¶lÃ§er
- Asimetrik bir metriktir (KL(P||Q) â‰  KL(Q||P))
- Information theory temelli bir yaklaÅŸÄ±mdÄ±r
- VAE ve GAN gibi Ã¼retici modellerde sÄ±kÃ§a kullanÄ±lÄ±r

## Loss Fonksiyonu SeÃ§im Kriterleri

| Problem Tipi | Ã–nerilen Loss Fonksiyonu | KullanÄ±m AlanÄ± | Ã–zel Durumlar |
|--------------|-------------------------|----------------|----------------|
| Regresyon | MSE, MAE, Huber | SayÄ±sal tahmin | AykÄ±rÄ± deÄŸer varsa MAE veya Huber tercih edilebilir |
| Binary Classification | Binary Cross-Entropy, Focal Loss | Ä°kili sÄ±nÄ±flandÄ±rma | Dengesiz veri setlerinde Focal Loss tercih edilebilir |
| Multi-class Classification | Categorical Cross-Entropy | Ã‡oklu sÄ±nÄ±flandÄ±rma | Sparse versiyonu bellek tasarrufu saÄŸlar |
| Probabilistic Models | KL Divergence | OlasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ± | Autoencoder ve GAN'larda sÄ±kÃ§a kullanÄ±lÄ±r |

### Loss DeÄŸiÅŸiminin GÃ¶rselleÅŸtirmesi:

```
     Loss
     ^
     |
     |    .  Training Loss
     |      .
     |        .  Validation Loss
     |          .
     |            ..
     |              ...
     |                 ....
     +--------------------------> Epochs
```

## Ã–nemli Notlar ve En Ä°yi Uygulamalar:
- Loss deÄŸerinin mutlak bÃ¼yÃ¼klÃ¼ÄŸÃ¼ deÄŸil, eÄŸitim boyunca gÃ¶sterdiÄŸi eÄŸilim Ã¶nemlidir
- Validation loss'un training loss'tan belirgin ÅŸekilde yÃ¼ksek olmasÄ± overfitting gÃ¶stergesidir
- Loss fonksiyonu seÃ§imi, problemin doÄŸasÄ± ve veri daÄŸÄ±lÄ±mÄ±na uygun olmalÄ±dÄ±r
- Ã–zel durumlarda birden fazla loss fonksiyonu birleÅŸtirilebilir (custom loss)
- Loss deÄŸerlerinin Ã¶lÃ§eklendirilmesi ve normalizasyonu model performansÄ±nÄ± etkileyebilir
- Early stopping iÃ§in validation loss takip edilmelidir

## KullanÄ±lan TÃ¼m KayÄ±p FonksiyonlarÄ±nÄ±n Listesi

| KullanÄ±m AlanÄ±                    | KayÄ±p Fonksiyonu                            | AÃ§Ä±klama                                                                 |
|-----------------------------------|---------------------------------------------|--------------------------------------------------------------------------|
| **SÄ±nÄ±flandÄ±rma**                 | **Binary Crossentropy**                     | Ä°kili sÄ±nÄ±flandÄ±rma problemlerinde kullanÄ±lÄ±r.                          |
|                                   | **Categorical Crossentropy**                | One-hot encoded etiketlerle birlikte kullanÄ±lÄ±r.                         |
|                                   | **Sparse Categorical Crossentropy**        | SÄ±nÄ±f indeksleri ile etiketleme iÃ§in kullanÄ±lÄ±r.                        |
|                                   | **Focal Loss**                              | Dengesiz veri setlerinde daha fazla odaklanmak iÃ§in kullanÄ±lÄ±r.        |
|                                   | **Cohen's Kappa Loss**                      | SÄ±nÄ±flar arasÄ±ndaki dengesizliÄŸi dikkate alÄ±r.                          |
|                                   | **Softmax Loss**                            | Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma iÃ§in kullanÄ±lÄ±r.                              |
|                                   | **Contrastive Loss**                        | Ä°ki Ã¶rnek arasÄ±ndaki mesafeyi optimize eder.                           |
|                                   | **Triplet Loss**                            | ÃœÃ§lÃ¼ Ã¶rnekler arasÄ±ndaki mesafeyi optimize eder.                       |
| **Regresyon**                     | **Mean Squared Error (MSE)**                | GerÃ§ek ve tahmin edilen deÄŸerler arasÄ±ndaki ortalama kare farkÄ±nÄ± hesaplar. |
|                                   | **Mean Absolute Error (MAE)**               | GerÃ§ek ve tahmin edilen deÄŸerler arasÄ±ndaki ortalama mutlak farkÄ± hesaplar. |
|                                   | **Huber Loss**                              | MSE ve MAE'nin bir kombinasyonu, hata kÃ¼Ã§Ã¼kse MSE, bÃ¼yÃ¼kse MAE kullanÄ±r. |
|                                   | **Poisson Loss**                            | SayÄ±m verileriyle Ã§alÄ±ÅŸÄ±rken kullanÄ±lÄ±r.                                |
| **GÃ¶rÃ¼ntÃ¼ Segmentasyonu**        | **Tversky Loss**                            | YanlÄ±ÅŸ pozitif ve negatif hatalarÄ± ayÄ±rt eder.                         |
|                                   | **Jaccard Loss**                            | Ä°ki kÃ¼me arasÄ±ndaki benzerliÄŸi Ã¶lÃ§er.                                  |
| **OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±** | **Kullback-Leibler Divergence**         | Ä°ki olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± arasÄ±ndaki farkÄ± Ã¶lÃ§er.                          |
| **Ã–zel Uygulamalar**             | **Custom Loss Functions**                   | KullanÄ±cÄ±lar tarafÄ±ndan belirli bir probleme gÃ¶re Ã¶zelleÅŸtirilmiÅŸ kayÄ±plar. |
|                                   | **Adversarial Loss**                        | Ãœretici ve ayÄ±rt edici aÄŸlar arasÄ±nda denge kurar.                     |
|                                   | **Weighted Loss**                           | Belirli sÄ±nÄ±flara daha fazla aÄŸÄ±rlÄ±k vermek iÃ§in tasarlanmÄ±ÅŸtÄ±r.       |

