# LSTM ve GRU (Long Short-Term Memory & Gated Recurrent Unit)

## ğŸ“ BÃ¶lÃ¼m HaritasÄ±
- Ã–nceki BÃ¶lÃ¼m: [RNN](02-RNN.md)
- Sonraki BÃ¶lÃ¼m: [Transformers](04-Transformers.md)
- Tahmini SÃ¼re: 5-6 saat
- Zorluk Seviyesi: ğŸŸ¡ Orta

## ğŸ¯ Hedefler
- LSTM ve GRU mimarilerini anlama
- Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenme
- KapÄ± mekanizmalarÄ±nÄ± kavrama
- Modern uygulamalarÄ± geliÅŸtirme

## ï¿½ï¿½ Ã–z DeÄŸerlendirme
- [ ] LSTM ve GRU yapÄ±larÄ±nÄ± aÃ§Ä±klayabiliyorum
- [ ] KapÄ± mekanizmalarÄ±nÄ± anlayabiliyorum
- [ ] FarklÄ± mimarileri karÅŸÄ±laÅŸtÄ±rabiliyorum
- [ ] Uygun mimariye karar verebiliyorum

## ğŸš€ Mini Projeler
1. Dil Modeli
   - LSTM tabanlÄ± dil modeli
   - FarklÄ± kapÄ± yapÄ±larÄ±
   - Performans analizi

2. MÃ¼zik Ãœretimi
   - MIDI dosya iÅŸleme
   - GRU tabanlÄ± model
   - Stil transferi

## ğŸ“‘ Ã–n KoÅŸullar
- RNN mimarisi bilgisi
- Python ve derin Ã¶ÄŸrenme framework'leri
- Gradyan akÄ±ÅŸÄ± kavramlarÄ±
- Optimizasyon teknikleri

## ï¿½ï¿½ Temel Kavramlar
1. KapÄ± MekanizmalarÄ±
2. HÃ¼cre Durumu
3. Uzun Vadeli BaÄŸÄ±mlÄ±lÄ±klar
4. Gradyan KontrolÃ¼

## GiriÅŸ
ğŸŸ¡ Orta

LSTM ve GRU, RNN'lerin uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenmedeki problemlerini Ã§Ã¶zmek iÃ§in geliÅŸtirilmiÅŸ Ã¶zel mimari yapÄ±lardÄ±r.

### Neden LSTM/GRU?
- Gradyan kaybÄ±/patlamasÄ± problemini Ã§Ã¶zme
- Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± yakalayabilme
- SeÃ§ici bilgi akÄ±ÅŸÄ± kontrolÃ¼

## LSTM'in YapÄ±sÄ±
ğŸ’¡ Ä°pucu: LSTM'in Ã¼Ã§ kapÄ±sÄ± vardÄ±r: forget, input ve output

### 1. LSTM HÃ¼cresi

![LSTM Structure](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

1. **Forget Gate**: Hangi bilgilerin unutulacaÄŸÄ±na karar verir
2. **Input Gate**: Hangi yeni bilgilerin saklanacaÄŸÄ±na karar verir
3. **Cell State**: Uzun vadeli bilgiyi taÅŸÄ±r
4. **Output Gate**: Hangi bilgilerin Ã§Ä±ktÄ± olarak verileceÄŸine karar verir

### LSTM Implementasyonu

```python
import tensorflow as tf

# Basit LSTM modeli
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64, return_sequences=True, 
                        input_shape=(timesteps, features)),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Ã‡ift yÃ¶nlÃ¼ LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(64, return_sequences=True),
        input_shape=(timesteps, features)
    ),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

### LSTM KatmanÄ± Ã–zellikleri

```python
lstm_layer = tf.keras.layers.LSTM(
    units=64,                    # Gizli birim sayÄ±sÄ±
    activation='tanh',           # Ana aktivasyon fonksiyonu
    recurrent_activation='sigmoid', # Ä°Ã§ kapÄ± aktivasyonlarÄ±
    return_sequences=True,       # TÃ¼m zaman adÄ±mlarÄ± iÃ§in Ã§Ä±ktÄ±
    return_state=False,          # Durum vektÃ¶rlerini dÃ¶ndÃ¼rme
    stateful=False,              # Durum hafÄ±zasÄ±
    dropout=0.2,                 # GiriÅŸ dropout oranÄ±
    recurrent_dropout=0.2        # Tekrarlayan baÄŸlantÄ± dropout oranÄ±
)
```

## GRU (Gated Recurrent Unit)

GRU, LSTM'in daha basit bir versiyonudur ve daha az parametre iÃ§erir.

### GRU'nun YapÄ±sÄ±

![GRU Structure](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)

1. **Update Gate**: LSTM'deki forget ve input gate'lerin birleÅŸimi
2. **Reset Gate**: Ã–nceki durumun ne kadarÄ±nÄ±n unutulacaÄŸÄ±nÄ± belirler

### GRU Implementasyonu

```python
# Basit GRU modeli
model = tf.keras.Sequential([
    tf.keras.layers.GRU(64, return_sequences=True,
                       input_shape=(timesteps, features)),
    tf.keras.layers.GRU(32),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Ã‡ift yÃ¶nlÃ¼ GRU
model = tf.keras.Sequential([
    tf.keras.layers.Bidirectional(
        tf.keras.layers.GRU(64, return_sequences=True),
        input_shape=(timesteps, features)
    ),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

## LSTM vs GRU KarÅŸÄ±laÅŸtÄ±rmasÄ±

```python
# AynÄ± problem iÃ§in her iki model
lstm_model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1)
])

gru_model = tf.keras.Sequential([
    tf.keras.layers.GRU(64),
    tf.keras.layers.Dense(1)
])

# Model derleme
lstm_model.compile(optimizer='adam', loss='mse')
gru_model.compile(optimizer='adam', loss='mse')

# EÄŸitim ve karÅŸÄ±laÅŸtÄ±rma
lstm_history = lstm_model.fit(X_train, y_train, validation_split=0.2, epochs=100)
gru_history = gru_model.fit(X_train, y_train, validation_split=0.2, epochs=100)

# Performans karÅŸÄ±laÅŸtÄ±rma grafiÄŸi
plt.plot(lstm_history.history['loss'], label='LSTM Training Loss')
plt.plot(gru_history.history['loss'], label='GRU Training Loss')
plt.legend()
plt.show()
```

## Ã–rnek Uygulamalar

### 1. Metin SÄ±nÄ±flandÄ±rma
```python
# Metin sÄ±nÄ±flandÄ±rma modeli
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim),
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])
```

### 2. Zaman Serisi Tahmini
```python
# Zaman serisi modeli
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(50, return_sequences=True,
                        input_shape=(n_steps, n_features)),
    tf.keras.layers.LSTM(50),
    tf.keras.layers.Dense(1)
])
```

### 3. Makine Ã‡evirisi
```python
def build_nmt_model(src_vocab_size, tgt_vocab_size, embedding_dim=256):
    # Encoder
    encoder_inputs = tf.keras.layers.Input(shape=(None,))
    encoder_embedding = tf.keras.layers.Embedding(src_vocab_size, embedding_dim)(encoder_inputs)
    
    # Bidirectional LSTM encoder
    encoder = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(embedding_dim, return_state=True)
    )
    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_embedding)
    
    # BirleÅŸtirme
    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])
    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])
    encoder_states = [state_h, state_c]
    
    # Decoder
    decoder_inputs = tf.keras.layers.Input(shape=(None,))
    decoder_embedding = tf.keras.layers.Embedding(tgt_vocab_size, embedding_dim)(decoder_inputs)
    decoder_lstm = tf.keras.layers.LSTM(embedding_dim*2, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
    
    # Attention mekanizmasÄ±
    attention = tf.keras.layers.Attention()
    context_vector = attention([decoder_outputs, encoder_outputs])
    
    # Ã‡Ä±ktÄ± katmanÄ±
    decoder_dense = tf.keras.layers.Dense(tgt_vocab_size, activation='softmax')
    outputs = decoder_dense(decoder_outputs)
    
    return tf.keras.Model([encoder_inputs, decoder_inputs], outputs)
```

### 4. Finansal Tahmin
```python
def build_stock_predictor():
    model = tf.keras.Sequential([
        # Feature extraction
        tf.keras.layers.LSTM(128, return_sequences=True, 
                           input_shape=(lookback, n_features)),
        tf.keras.layers.Dropout(0.2),
        
        # Market pattern analysis
        tf.keras.layers.LSTM(256, return_sequences=True),
        tf.keras.layers.Dropout(0.2),
        
        # Long-term dependencies
        tf.keras.layers.GRU(128, return_sequences=False),
        
        # Price prediction
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(prediction_horizon)  # Ã‡oklu zaman adÄ±mÄ± tahmini
    ])
    return model
```

### 5. Video Analizi
```python
def build_video_classifier():
    # CNN Ã¶zellik Ã§Ä±karÄ±cÄ±
    cnn_base = tf.keras.applications.ResNet50(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3)
    )
    cnn_base.trainable = False
    
    # Video sÄ±nÄ±flandÄ±rma modeli
    model = tf.keras.Sequential([
        tf.keras.layers.TimeDistributed(cnn_base),
        tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAveragePooling2D()),
        
        # Temporal Ã¶zellik Ã§Ä±karma
        tf.keras.layers.LSTM(512, return_sequences=True),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.GRU(256),
        
        # SÄ±nÄ±flandÄ±rma
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    return model
```

### 6. Ses TanÄ±ma
```python
def build_speech_recognizer():
    # Spektrogram giriÅŸi
    inputs = tf.keras.layers.Input(shape=(None, n_mels))
    
    # Ã–zellik Ã§Ä±karma
    x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv1D(256, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    
    # Temporal modelleme
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(x)
    
    # CTC loss iÃ§in Ã§Ä±ktÄ±
    outputs = tf.keras.layers.Dense(num_characters + 1, activation='softmax')(x)
    
    return tf.keras.Model(inputs=inputs, outputs=outputs)
```

## GÃ¶rselleÅŸtirme Ã–rnekleri

### 1. Gate AktivasyonlarÄ±
```python
def visualize_gate_activations(model, input_sequence):
    # Gate aktivasyonlarÄ±nÄ± almak iÃ§in Ã¶zel model
    gate_model = tf.keras.Model(
        inputs=model.input,
        outputs=[
            model.get_layer('lstm').cell.input_gate,
            model.get_layer('lstm').cell.forget_gate,
            model.get_layer('lstm').cell.output_gate
        ]
    )
    
    # Gate aktivasyonlarÄ±nÄ± hesapla
    i_gate, f_gate, o_gate = gate_model.predict(input_sequence)
    
    # GÃ¶rselleÅŸtirme
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.imshow(i_gate.T, aspect='auto', cmap='viridis')
    plt.title('Input Gate Activations')
    plt.xlabel('Time Step')
    plt.ylabel('Hidden Unit')
    
    plt.subplot(1, 3, 2)
    plt.imshow(f_gate.T, aspect='auto', cmap='viridis')
    plt.title('Forget Gate Activations')
    plt.xlabel('Time Step')
    
    plt.subplot(1, 3, 3)
    plt.imshow(o_gate.T, aspect='auto', cmap='viridis')
    plt.title('Output Gate Activations')
    plt.xlabel('Time Step')
    
    plt.tight_layout()
    plt.show()
```

### 2. Memory Cell Analizi
```python
def analyze_memory_cells(model, input_sequence):
    # Memory cell durumlarÄ±nÄ± almak iÃ§in Ã¶zel model
    memory_model = tf.keras.Model(
        inputs=model.input,
        outputs=model.get_layer('lstm').cell.state_h
    )
    
    # Memory cell durumlarÄ±nÄ± hesapla
    cell_states = memory_model.predict(input_sequence)
    
    # SeÃ§ili hÃ¼creleri gÃ¶rselleÅŸtir
    plt.figure(figsize=(12, 6))
    for i in range(min(5, cell_states.shape[-1])):
        plt.plot(cell_states[:, i], label=f'Cell {i}')
    
    plt.title('Memory Cell States Over Time')
    plt.xlabel('Time Step')
    plt.ylabel('Cell State Value')
    plt.legend()
    plt.grid(True)
    plt.show()
```

### 3. LSTM vs GRU KarÅŸÄ±laÅŸtÄ±rmasÄ±
```python
def compare_lstm_gru_performance(X_train, y_train, epochs=50):
    # LSTM ve GRU modelleri
    lstm_model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
        tf.keras.layers.Dense(1)
    ])
    
    gru_model = tf.keras.Sequential([
        tf.keras.layers.GRU(64, input_shape=(X_train.shape[1], X_train.shape[2])),
        tf.keras.layers.Dense(1)
    ])
    
    # Modelleri derle
    lstm_model.compile(optimizer='adam', loss='mse')
    gru_model.compile(optimizer='adam', loss='mse')
    
    # EÄŸitim
    lstm_history = lstm_model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)
    gru_history = gru_model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)
    
    # Performans karÅŸÄ±laÅŸtÄ±rmasÄ±
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(lstm_history.history['loss'], label='LSTM Training')
    plt.plot(lstm_history.history['val_loss'], label='LSTM Validation')
    plt.plot(gru_history.history['loss'], label='GRU Training')
    plt.plot(gru_history.history['val_loss'], label='GRU Validation')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.tight_layout()
    plt.show()
```

### 4. Uzun Vadeli BaÄŸÄ±mlÄ±lÄ±k Analizi
```python
def analyze_long_term_dependencies(model, input_sequence, target_time_step):
    # Gradyan analizi iÃ§in Ã¶zel fonksiyon
    @tf.function
    def get_gradients(input_seq):
        with tf.GradientTape() as tape:
            tape.watch(input_seq)
            output = model(input_seq)
            target_output = output[:, target_time_step, :]
        return tape.gradient(target_output, input_seq)
    
    # GradyanlarÄ± hesapla
    grads = get_gradients(input_sequence)
    grad_magnitudes = tf.reduce_mean(tf.abs(grads), axis=-1)
    
    # GÃ¶rselleÅŸtirme
    plt.figure(figsize=(10, 6))
    plt.plot(grad_magnitudes[0])
    plt.title(f'Gradient Magnitudes for Target Time Step {target_time_step}')
    plt.xlabel('Input Time Step')
    plt.ylabel('Average Gradient Magnitude')
    plt.grid(True)
    plt.show()
```

### 5. Ã–zellik Ã–nem Analizi
```python
def visualize_feature_importance(model, X_test, feature_names):
    # SHAP deÄŸerleri hesapla
    import shap
    explainer = shap.DeepExplainer(model, X_test[:100])
    shap_values = explainer.shap_values(X_test[:1000])
    
    # Ã–zellik Ã¶nemlerini gÃ¶rselleÅŸtir
    plt.figure(figsize=(12, 6))
    shap.summary_plot(shap_values[0], X_test[:1000], feature_names=feature_names)
    plt.title('Feature Importance Analysis')
    plt.show()
```

## ğŸ“š Ã–nerilen Kaynaklar
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Illustrated Guide to LSTM's and GRU's](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
- [TensorFlow LSTM Guide](https://www.tensorflow.org/guide/keras/rnn#lstm_layers)

## âœï¸ AlÄ±ÅŸtÄ±rmalar
### BaÅŸlangÄ±Ã§ Seviyesi
1. Basit zaman serisi tahmini
2. Duygu analizi uygulamasÄ±

### Orta Seviye
1. MÃ¼zik Ã¼retimi
2. Makine Ã§evirisi

### Ä°leri Seviye
1. Custom LSTM/GRU hÃ¼cresi tasarlama
2. Attention mekanizmasÄ± entegrasyonu 