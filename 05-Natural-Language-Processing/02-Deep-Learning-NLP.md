# Derin Ã–ÄŸrenmede NLP Modelleri

## 1. GiriÅŸ
DoÄŸal Dil Ä°ÅŸleme (NLP) alanÄ±nda derin Ã¶ÄŸrenme modelleri, metin verilerini analiz etme ve anlama konusunda harika sonuÃ§lar Ã¼retmiÅŸtir. Son yÄ±llarda, Ã¶zellikle Transformer mimarisi gibi geliÅŸmeler sayesinde, NLP alanÄ±nda Ã§Ä±ÄŸÄ±r aÃ§Ä±cÄ± ilerlemeler kaydedilmiÅŸtir.

DoÄŸal Dil Ä°ÅŸleme (NLP), gÃ¼nÃ¼mÃ¼z teknoloji dÃ¼nyasÄ±nda kritik bir role sahiptir. **Ä°nsan ve makine arasÄ±ndaki iletiÅŸimi geliÅŸtirerek** daha doÄŸal ve etkili etkileÅŸimler saÄŸlar. Ã–zellikle bÃ¼yÃ¼k metin verilerinden **anlamlÄ± bilgiler Ã§Ä±karma** konusunda gÃ¼Ã§lÃ¼ yeteneklere sahiptir. Otomatik **dil Ã§evirisi** ve **metin Ã¶zetleme** gibi pratik uygulamalarla gÃ¼nlÃ¼k hayatÄ± kolaylaÅŸtÄ±rÄ±rken, **duygu analizi** ve **metin sÄ±nÄ±flandÄ±rma** gibi ileri dÃ¼zey analizlerle de iÅŸ sÃ¼reÃ§lerine deÄŸer katar. Bu teknolojiler, hem bireysel kullanÄ±cÄ±larÄ±n hem de kurumsal sistemlerin verimliliÄŸini artÄ±rarak, modern dijital dÃ¶nÃ¼ÅŸÃ¼mÃ¼n temel yapÄ± taÅŸlarÄ±ndan birini oluÅŸturur.

Bu teknoloji, insan **dilinin karmaÅŸÄ±k yapÄ±larÄ±nÄ± anlama** ve iÅŸleme konusunda olaÄŸanÃ¼stÃ¼ yetenekler sergilemektedir. Geleneksel yÃ¶ntemlerin aksine, derin Ã¶ÄŸrenme modelleri veriyi ham formundan Ã§Ä±ktÄ±ya kadar **uÃ§tan uca (end-to-end) iÅŸleyebilmektedir**. Ã–zellikle **baÄŸlam tabanlÄ± kelime temsilleri** sayesinde, kelimelerin farklÄ± anlamlarÄ±nÄ± ve kullanÄ±mlarÄ±nÄ± baÅŸarÄ±yla yakalayabilmektedir. Bu geliÅŸmiÅŸ Ã¶zellikler, Ã§eÅŸitli NLP gÃ¶revlerinde **yÃ¼ksek doÄŸrulukta sonuÃ§lar** elde edilmesini saÄŸlamÄ±ÅŸtÄ±r.


## 2. Temel NLP Modelleri

### 2.1 Word Embeddings (Kelime GÃ¶mme)
Word Embeddings (Kelime GÃ¶mme), kelimeleri bilgisayarlarÄ±n anlayabileceÄŸi sayÄ±sal deÄŸerlere dÃ¶nÃ¼ÅŸtÃ¼rme yÃ¶ntemidir. Bunu gÃ¼nlÃ¼k hayattan bir Ã¶rnekle aÃ§Ä±klayalÄ±m:

**ğŸ—ºï¸ Benzetme:** Bir ÅŸehir haritasÄ±nda her lokasyonun koordinatlarla (x,y) temsil edilmesi gibi, Word Embeddings de her kelimeyi Ã§ok boyutlu bir uzayda koordinatlarla temsil eder.

**Temel Ã–zellikleri:**
* Her kelime bir vektÃ¶r (sayÄ± dizisi) ile temsil edilir
* Benzer anlamlÄ± kelimeler, vektÃ¶r uzayÄ±nda birbirine yakÄ±n konumlanÄ±r
* Kelimeler arasÄ±ndaki iliÅŸkiler matematiksel olarak hesaplanabilir

**Ã–rnek:**

```python
# Word Embedding Ã¶rneÄŸi
kral = [0.2, 0.5, -0.3, 0.8]
kraliÃ§e = [0.1, 0.6, -0.2, 0.9]
adam = [-0.4, 0.3, 0.2, 0.1]
kadÄ±n = [-0.3, 0.4, 0.3, 0.2]
```

Bu Ã¶rnekte her kelime 4 boyutlu bir vektÃ¶rle temsil edilmiÅŸtir. Benzer kelimeler (kral-kraliÃ§e veya adam-kadÄ±n) birbirine yakÄ±n deÄŸerler alÄ±r.

```python
# Word Embeddings Ã¶rneÄŸi
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

# Ã–rnek metin
text = ["derin Ã¶ÄŸrenme harika", "nlp Ã§ok gÃ¼zel"]

# One-hot encoding
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(text)

# VektÃ¶r gÃ¶sterimi
print("Kelime vektÃ¶rleri:")
print(X.toarray())
print("\nKelime listesi:", vectorizer.get_feature_names_out())
```
#### Word2Vec
Word2Vec, kelime gÃ¶mme Ã¶ÄŸrenmek iÃ§in kullanÄ±lan popÃ¼ler bir tekniktir. Ä°ki farklÄ± model mimarisi (CBOW ve Skip-gram) kullanarak kelimelerin vektÃ¶r temsillerini Ã¶ÄŸrenir. 

- CBOW (Continuous Bag of Words): CBOW modeli, baÄŸlam kelimelerden hedef kelimeyi tahmin eder
- Skip-gram modeli: CBOW modelinin tersini yapar
- Negative Sampling: EÄŸitim sÃ¼recini hÄ±zlandÄ±rmak iÃ§in kullanÄ±lan etkili bir optimizasyon tekniÄŸidir

```python
from gensim.models import Word2Vec

# Ã–rnek veri
sentences = [["ben", "nlp", "Ã¶ÄŸreniyorum"], ["derin", "Ã¶ÄŸrenme", "harika"]]

# Word2Vec modeli
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# Benzer kelimeleri bulma
similar_words = model.wv.most_similar('nlp')
```

#### GloVe (Global Vectors)
GloVe, kelime vektÃ¶rlerini oluÅŸturmak iÃ§in kelime eÅŸ-oluÅŸum istatistiklerini kullanan gÃ¼Ã§lÃ¼ bir kelime gÃ¶mme yÃ¶ntemidir. Word2Vec'ten farklÄ± olarak, global matris faktÃ¶rizasyonu yaklaÅŸÄ±mÄ±nÄ± kullanarak tÃ¼m korpustaki kelime iliÅŸkilerini bir arada deÄŸerlendirir. Bu sayede kelimelerin semantik ve sÃ¶zdizimsel Ã¶zelliklerini daha iyi yakalayabilir.

- Global matris faktÃ¶rizasyonu
- EÅŸ-oluÅŸum olasÄ±lÄ±klarÄ±

```python
from torchtext.vocab import GloVe

# GloVe vektÃ¶rlerini yÃ¼kleme
glove = GloVe(name='6B', dim=100)
```

#### FastText
FastText, Facebook tarafÄ±ndan geliÅŸtirilen ve Word2Vec'in geliÅŸtirilmiÅŸ bir versiyonu olan kelime gÃ¶mme yÃ¶ntemidir. Kelimeleri karakter n-gramlarÄ±na bÃ¶lerek iÅŸler, bu sayede bilinmeyen veya nadir kelimelerin vektÃ¶r temsillerini de oluÅŸturabilir. Bu Ã¶zelliÄŸi sayesinde, Ã¶zellikle morfolojik aÃ§Ä±dan zengin dillerde etkili sonuÃ§lar Ã¼retir.

- Karakter n-gram'larÄ±
- OOV (Out-of-Vocabulary) kelime desteÄŸi

```python
from gensim.models import FastText

# FastText modeli
model = FastText(sentences, vector_size=100, window=5, min_count=1)
```

#### Uygulama AlanlarÄ±
- Semantik benzerlik analizi
- Metin sÄ±nÄ±flandÄ±rma
- Makine Ã§evirisi
- Duygu analizi

### 2.2 Recurrent Neural Networks (RNN)
Tekrarlayan Sinir AÄŸlarÄ± (RNN), Ã¶zellikle metin, ses ve zaman serisi gibi sÄ±ralÄ± verileri iÅŸlemek iÃ§in tasarlanmÄ±ÅŸ Ã¶zel bir yapay sinir aÄŸÄ± mimarisidir. Geleneksel sinir aÄŸlarÄ±ndan farklÄ± olarak, RNN'ler Ã¶nceki adÄ±mlardaki bilgileri hafÄ±zada tutarak, mevcut girdiyi iÅŸlerken geÃ§miÅŸ bilgileri de kullanabilir.

**Temel Ã–zellikleri:**
* SÄ±ralÄ± veri iÅŸleme yeteneÄŸi
* Dahili hafÄ±za mekanizmasÄ±
* Her adÄ±mda aynÄ± parametreleri kullanma
* DeÄŸiÅŸken uzunluktaki girdileri iÅŸleyebilme

#### RNN'lerin Temel YapÄ±sÄ±
- GiriÅŸ KatmanÄ± (Input Layer)
- Gizli Katman (Hidden Layer)
- Ã‡Ä±kÄ±ÅŸ KatmanÄ± (Output Layer)
- Geri Besleme BaÄŸlantÄ±larÄ± (Feedback Connections)

#### RNN TÃ¼rleri
- Tek YÃ¶nlÃ¼ RNN (Unidirectional RNN)
- Ã‡ift YÃ¶nlÃ¼ RNN (Bidirectional RNN)
- Ã‡ok KatmanlÄ± RNN (Multi-layer RNN)

```python
import tensorflow as tf

# Basit RNN modeli
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 64),
    tf.keras.layers.SimpleRNN(128),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Ã‡ift YÃ¶nlÃ¼ RNN Ã¶rneÄŸi
bidirectional_model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(128)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
#### RNN'lerin AvantajlarÄ±
- DeÄŸiÅŸken uzunluktaki dizileri iÅŸleyebilme
- Ã–nceki bilgileri hafÄ±zada tutabilme
- SÄ±ralÄ± verilerde baÅŸarÄ±lÄ± sonuÃ§lar

#### RNN'lerin ZorluklarÄ±
- Vanishing Gradient Problemi
- Uzun Vadeli BaÄŸÄ±mlÄ±lÄ±klarÄ± Ã–ÄŸrenememe
- YavaÅŸ EÄŸitim SÃ¼reci


#### RNN Uygulama AlanlarÄ±
- Metin SÄ±nÄ±flandÄ±rma
- Dil Modelleme
- KonuÅŸma TanÄ±ma
- Makine Ã‡evirisi
- Zaman Serisi Analizi

## 3. Modern NLP Mimarileri

### 3.1 LSTM ve GRU
LSTM (Long Short-Term Memory) ve GRU (Gated Recurrent Unit), RNN'lerin vanishing gradient problemini Ã§Ã¶zmek iÃ§in geliÅŸtirilmiÅŸ Ã¶zel mimari yapÄ±lardÄ±r.

**Temel AvantajlarÄ±:**
* Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilme
* Vanishing gradient problemini Ã§Ã¶zme
* SeÃ§ici bilgi akÄ±ÅŸÄ± kontrolÃ¼
* Adaptif hafÄ±za mekanizmasÄ±

**ğŸ”„ Benzetme:** LSTM ve GRU'yu bir gÃ¼venlik kontrol noktasÄ±na benzetebiliriz:
- LSTM: DÃ¶rt farklÄ± kontrol noktasÄ± (gate) ile detaylÄ± gÃ¼venlik kontrolÃ¼
- GRU: Ä°ki kontrol noktasÄ± ile daha basit ama etkili kontrol

#### Temel YapÄ±larÄ±:
LSTM:
```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Forget â”‚
Input â”€â”€â–º  â”‚   Input â”‚  â”€â”€â–º Output
           â”‚ Output  â”‚
           â”‚   Cell  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

GRU:
```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Input â”€â”€â–º  â”‚ Update  â”‚  â”€â”€â–º Output
           â”‚  Reset  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



#### LSTM Mimarisi
- Forget Gate: Gereksiz bilgileri unutmayÄ± saÄŸlar
- Input Gate: Yeni bilgilerin hÃ¼creye giriÅŸini kontrol eder
- Output Gate: HÃ¼cre Ã§Ä±ktÄ±sÄ±nÄ± dÃ¼zenler
- Cell State: Uzun vadeli bilgileri saklar

```python
# LSTM modeli Ã¶rneÄŸi
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(10)
])
```

#### GRU Mimarisi
- Reset Gate: Ã–nceki bilgileri sÄ±fÄ±rlamayÄ± kontrol eder
- Update Gate: Yeni bilgilerin gÃ¼ncellenmesini yÃ¶netir

```python
# GRU modeli Ã¶rneÄŸi
model = tf.keras.Sequential([
    tf.keras.layers.GRU(64, return_sequences=True),
    tf.keras.layers.GRU(32),
    tf.keras.layers.Dense(10)
])
```

#### LSTM ve GRU KarÅŸÄ±laÅŸtÄ±rmasÄ±
- GRU daha basit yapÄ± ve daha hÄ±zlÄ± eÄŸitim
- LSTM daha karmaÅŸÄ±k ve gÃ¼Ã§lÃ¼ hafÄ±za mekanizmasÄ±
- GRU daha az parametre ve bellek kullanÄ±mÄ±

#### Uygulama AlanlarÄ±
- Dizi-Dizi DÃ¶nÃ¼ÅŸÃ¼mleri
- Zaman Serisi Tahmini
- Metin OluÅŸturma
- MÃ¼zik Kompozisyonu

#### BaÅŸarÄ±lÄ± LSTM/GRU UygulamalarÄ±
- Dil Modellemesi
- KonuÅŸma TanÄ±ma
- Makine Ã‡evirisi
- El YazÄ±sÄ± TanÄ±ma

### 3.2 Transformer Mimarisi
Transformer mimarisi, 2017 yÄ±lÄ±nda tanÄ±tÄ±lan ve NLP alanÄ±nda devrim yaratan yeni bir yapay sinir aÄŸÄ± modelidir. Bu mimari, Ã¶nceki modellerin aksine RNN veya LSTM gibi tekrarlayan yapÄ±lar kullanmaz. Bunun yerine, "attention" (dikkat) mekanizmasÄ± ile Ã§alÄ±ÅŸÄ±r.

**ğŸ¯ Benzetme:** Transformer'Ä± bir sÄ±nÄ±ftaki Ã¶ÄŸrenci-Ã¶ÄŸretmen iliÅŸkisine benzetebiliriz:
- Ã–ÄŸretmen (attention mekanizmasÄ±) tÃ¼m Ã¶ÄŸrencilere (kelimelere) aynÄ± anda bakabilir
- Ã–nemli gÃ¶rdÃ¼ÄŸÃ¼ noktalara daha fazla dikkat eder
- Her Ã¶ÄŸrenciyi diÄŸer Ã¶ÄŸrencilerle iliÅŸkilendirerek deÄŸerlendirir

#### Ana BileÅŸenler

1. **Multi-head Attention (Ã‡oklu BaÅŸlÄ± Dikkat)**
    - Birden fazla dikkat mekanizmasÄ± paralel Ã§alÄ±ÅŸÄ±r
    - Her biri farklÄ± Ã¶zelliklere odaklanÄ±r
    - TÄ±pkÄ± bir konuyu farklÄ± aÃ§Ä±lardan incelemek gibi

2. **Positional Encoding (Konum Kodlama)**
    - Kelimelerin cÃ¼mle iÃ§indeki sÄ±rasÄ±nÄ± kodlar
    - Matematiksel sinÃ¼s ve kosinÃ¼s fonksiyonlarÄ± kullanÄ±r
    - BÃ¶ylece model, kelimelerin sÄ±rasÄ±nÄ± anlayabilir

3. **Feed-Forward Networks (Ä°leri Beslemeli AÄŸlar)**
    - Basit ama gÃ¼Ã§lÃ¼ sinir aÄŸÄ± katmanlarÄ±
    - ReLU aktivasyon fonksiyonu kullanÄ±r
    - Son iÅŸlemleri gerÃ§ekleÅŸtirir


#### Transformerin Ã‡alÄ±ÅŸma Prensibi

1. **GiriÅŸ AÅŸamasÄ±:**
    - Metin Ã¶nce kelimelere ayrÄ±lÄ±r
    - Her kelime sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
    - Konum bilgisi eklenir

2. **Ä°ÅŸleme AÅŸamasÄ±:**
    - Attention mekanizmasÄ± devreye girer
    - Her kelime diÄŸer kelimelerle iliÅŸkilendirilir
    - Ã–nemli baÄŸlantÄ±lar vurgulanÄ±r

3. **Ã‡Ä±kÄ±ÅŸ AÅŸamasÄ±:**
    - Son iÅŸlemler yapÄ±lÄ±r
    - SonuÃ§lar Ã¼retilir

#### AvantajlarÄ±
- Paralel iÅŸlem yapabilme (Ã§ok hÄ±zlÄ±)
- Uzun cÃ¼mleleri daha iyi anlama
- Kelimeler arasÄ± iliÅŸkileri iyi yakalama

#### DezavantajlarÄ±
- YÃ¼ksek hesaplama gÃ¼cÃ¼ gerektirir
- KarmaÅŸÄ±k bir yapÄ±ya sahiptir
- EÄŸitim iÃ§in Ã§ok veri gerektirir

#### AvantajlarÄ±
- Paralel iÅŸlem yeteneÄŸi
- Uzun mesafeli baÄŸÄ±mlÄ±lÄ±klarÄ± yakalama
- Ã–lÃ§eklenebilirlik
- Daha hÄ±zlÄ± eÄŸitim sÃ¼resi

#### Uygulama AlanlarÄ±
- Makine Ã§evirisi
- Metin Ã¶zetleme
- Dil modelleme
- Soru cevaplama sistemleri

## 4. BERT ve GPT Modelleri

### 4.1 BERT (Bidirectional Encoder Representations from Transformers)
#### Temel Ã–zellikleri
- Ã‡ift yÃ¶nlÃ¼ baÄŸlam anlama
- Masked Language Model (MLM)
- Next Sentence Prediction (NSP)
- WordPiece tokenization

#### BERT Mimarisi
- Transformer encoder katmanlarÄ±
- Multi-head attention
- Positional embeddings
- Segment embeddings

```python
from transformers import BertTokenizer, BertModel

# BERT model ve tokenizer yÃ¼kleme
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Ã–rnek kullanÄ±m
text = "NLP harika bir alan!"
encoded = tokenizer(text, return_tensors='pt')
outputs = model(**encoded)
```

### 4.2 GPT (Generative Pre-trained Transformer)
#### Temel Ã–zellikleri
- Tek yÃ¶nlÃ¼ (left-to-right) dil modeli
- Autoregressive yapÄ±
- BPE (Byte Pair Encoding) tokenization
- BÃ¼yÃ¼k Ã¶lÃ§ekli dil modelleme

#### GPT Mimarisi
- Transformer decoder katmanlarÄ±
- Masked self-attention
- BÃ¼yÃ¼k parametre sayÄ±sÄ±
- Scaling Ã¶zellikleri

```python
from transformers import GPT2Tokenizer, GPT2Model

# GPT model ve tokenizer yÃ¼kleme
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')

# Metin Ã¼retimi Ã¶rneÄŸi
input_text = "Yapay zeka"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model(**inputs)
```

### 4.3 Transfer Learning ve Fine-tuning
#### Pre-training AÅŸamasÄ±
- BÃ¼yÃ¼k veri setleri Ã¼zerinde eÄŸitim
- Genel dil anlama
- Self-supervised learning
- Domain-agnostic Ã¶ÄŸrenme

#### Fine-tuning Teknikleri
- Task-specific adaptasyon
- Learning rate optimizasyonu
- Gradient accumulation
- Layer freezing stratejileri

```python
from transformers import BertForSequenceClassification

# Fine-tuning iÃ§in model hazÄ±rlama
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Fine-tuning parametreleri
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5
)
```

### 4.4 Tokenization Stratejileri
#### WordPiece Tokenization
- Alt kelime birimleri
- Bilinmeyen kelime yÃ¶netimi
- Vocabulary optimizasyonu

#### BPE (Byte Pair Encoding)
- Karakter seviyesi tokenization
- SÄ±k kullanÄ±lan alt birimler
- Adaptif vocabulary

```python
# Tokenization Ã¶rnekleri
text = "Derin Ã¶ÄŸrenme ile doÄŸal dil iÅŸleme"

# BERT tokenization
bert_tokens = tokenizer.tokenize(text)

# Alt kelime analizi
token_ids = tokenizer.encode(text)
decoded = tokenizer.decode(token_ids)
```

## 5. Uygulama AlanlarÄ±
- Metin SÄ±nÄ±flandÄ±rma
- Duygu Analizi
- Makine Ã‡evirisi
- Soru Cevaplama

## 6. En Ä°yi Pratikler
- Veri Ã–n Ä°ÅŸleme
- Model SeÃ§imi
- Hiperparametre Optimizasyonu
- Model DeÄŸerlendirme

## 7. SonuÃ§
NLP alanÄ±ndaki derin Ã¶ÄŸrenme modelleri, basit Word Embeddings'lerden karmaÅŸÄ±k Transformer mimarilerine kadar geniÅŸ bir yelpazede geliÅŸim gÃ¶stermiÅŸtir. RNN'ler ve tÃ¼revleri (LSTM, GRU) sÄ±ralÄ± veri iÅŸlemede Ã¶nemli baÅŸarÄ±lar elde etmiÅŸ, ancak Transformer mimarisi ile birlikte alan yeni bir boyut kazanmÄ±ÅŸtÄ±r. BERT ve GPT gibi modern modeller, transfer learning yaklaÅŸÄ±mÄ±yla birlikte, doÄŸal dil iÅŸleme gÃ¶revlerinde state-of-the-art sonuÃ§lar elde etmeyi mÃ¼mkÃ¼n kÄ±lmÄ±ÅŸtÄ±r. Bu geliÅŸmeler, metin sÄ±nÄ±flandÄ±rma, makine Ã§evirisi, duygu analizi ve soru cevaplama gibi pratik uygulamalarda Ã¶nemli ilerlemeler saÄŸlamÄ±ÅŸtÄ±r.