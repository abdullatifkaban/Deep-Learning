# Aktivasyon FonksiyonlarÄ±

## ï¿½ï¿½ BÃ¶lÃ¼m HaritasÄ±
- Ã–nceki BÃ¶lÃ¼m: [Ã‡ok KatmanlÄ± AÄŸlar](02-Cok-Katmanli-Aglar.md)
- Sonraki BÃ¶lÃ¼m: [Geri YayÄ±lÄ±m](04-Geri-Yayilim.md)
- Tahmini SÃ¼re: 2-3 saat
- Zorluk Seviyesi: ğŸŸ¢ BaÅŸlangÄ±Ã§

## ğŸ¯ Hedefler
- Aktivasyon fonksiyonlarÄ±nÄ±n amacÄ±nÄ± anlama
- FarklÄ± fonksiyonlarÄ±n Ã¶zelliklerini Ã¶ÄŸrenme
- KullanÄ±m alanlarÄ±nÄ± ve seÃ§im kriterlerini kavrama
- TÃ¼revlerini ve gradyan hesaplamalarÄ±nÄ± anlama

## ğŸ¯ Ã–z DeÄŸerlendirme
- [ ] Aktivasyon fonksiyonlarÄ±nÄ±n amacÄ±nÄ± aÃ§Ä±klayabiliyorum
- [ ] FarklÄ± fonksiyonlarÄ±n avantaj/dezavantajlarÄ±nÄ± biliyorum
- [ ] Uygun fonksiyonu seÃ§ebiliyorum
- [ ] TÃ¼rev hesaplamalarÄ±nÄ± yapabiliyorum

## ğŸš€ Mini Projeler
1. Aktivasyon Fonksiyonu KarÅŸÄ±laÅŸtÄ±rmasÄ±
   - FarklÄ± fonksiyonlarÄ± gÃ¶rselleÅŸtirme
   - Gradyan deÄŸiÅŸimlerini analiz etme
   - Performans karÅŸÄ±laÅŸtÄ±rmasÄ±

2. Vanishing Gradient Problemi
   - Derin aÄŸlarda problem tespiti
   - Ã‡Ã¶zÃ¼m yÃ¶ntemleri uygulama
   - SonuÃ§larÄ± deÄŸerlendirme

## ğŸ“‘ Ã–n KoÅŸullar
...

## Aktivasyon FonksiyonlarÄ±nÄ±n Ã–nemi

Aktivasyon fonksiyonlarÄ±, yapay sinir aÄŸlarÄ±nda doÄŸrusal olmayan iliÅŸkileri modellemek iÃ§in kullanÄ±lan matematiksel fonksiyonlardÄ±r. Bu fonksiyonlar, nÃ¶ronlarÄ±n Ã§Ä±ktÄ±larÄ±nÄ± belirli bir aralÄ±ÄŸa sÄ±kÄ±ÅŸtÄ±rÄ±r ve aÄŸÄ±n Ã¶ÄŸrenme kapasitesini artÄ±rÄ±r.

## Temel Aktivasyon FonksiyonlarÄ±

### 1. Sigmoid Fonksiyonu

$$ f(x) = \frac{1}{1 + e^{-x}} $$

```python
import tensorflow as tf

def sigmoid(x):
    return tf.math.sigmoid(x)

# Keras ile kullanÄ±m
activation = tf.keras.layers.Activation('sigmoid')
# veya
layer = tf.keras.layers.Dense(units=10, activation='sigmoid')
```

![Sigmoid](https://miro.medium.com/max/1400/1*Xu7B5y9gp0iL5ooBj7LtWw.png)

**Ã–zellikleri:**
- Ã‡Ä±ktÄ± aralÄ±ÄŸÄ±: (0,1)
- SÃ¼rekli ve tÃ¼revlenebilir
- Gradyan kaybÄ± problemi yaÅŸanabilir
- Ä°kili sÄ±nÄ±flandÄ±rma problemlerinde Ã§Ä±kÄ±ÅŸ katmanÄ±nda kullanÄ±lÄ±r

### 2. ReLU (Rectified Linear Unit)

$$ f(x) = \max(0, x) $$

```python
def relu(x):
    return tf.nn.relu(x)

# Keras ile kullanÄ±m
activation = tf.keras.layers.ReLU()
# veya
layer = tf.keras.layers.Dense(units=10, activation='relu')
```

![ReLU](https://miro.medium.com/max/1400/1*XxxiA0jJvPrHEJHD4z893g.png)

**Ã–zellikleri:**
- Hesaplama aÃ§Ä±sÄ±ndan verimli
- Gradyan kaybÄ± problemini azaltÄ±r
- Ã–lÃ¼ ReLU problemi yaÅŸanabilir
- En yaygÄ±n kullanÄ±lan aktivasyon fonksiyonu

### 3. Tanh (Hiperbolik Tanjant)

$$ f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

```python
def tanh(x):
    return tf.math.tanh(x)

# Keras ile kullanÄ±m
activation = tf.keras.layers.Activation('tanh')
# veya
layer = tf.keras.layers.Dense(units=10, activation='tanh')
```

![Tanh](https://miro.medium.com/max/1400/1*f9erByySVjTjohfFdNkJYQ.png)

**Ã–zellikleri:**
- Ã‡Ä±ktÄ± aralÄ±ÄŸÄ±: (-1,1)
- Sigmoid'e gÃ¶re daha iyi gradyan akÄ±ÅŸÄ±
- SÄ±fÄ±r merkezli

### 4. Leaky ReLU

$$ f(x) = \max(\alpha x, x) $$

```python
def leaky_relu(x, alpha=0.01):
    return tf.nn.leaky_relu(x, alpha=alpha)

# Keras ile kullanÄ±m
activation = tf.keras.layers.LeakyReLU(alpha=0.01)
```

![Leaky ReLU](https://miro.medium.com/max/1400/1*A_Bzn0CjUgOXtPCJKnKLqA.png)

**Ã–zellikleri:**
- ReLU'nun Ã¶lÃ¼ nÃ¶ron problemini Ã§Ã¶zer
- Negatif deÄŸerler iÃ§in kÃ¼Ã§Ã¼k bir eÄŸim saÄŸlar
- Î± parametresi ayarlanabilir

### 5. Softmax

$$ f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}} $$

```python
def softmax(x):
    return tf.nn.softmax(x)

# Keras ile kullanÄ±m
activation = tf.keras.layers.Activation('softmax')
# veya
layer = tf.keras.layers.Dense(units=num_classes, activation='softmax')
```

![Softmax](https://miro.medium.com/max/1400/1*670CdxchunD-yAuUWdI7Bg.png)

**Ã–zellikleri:**
- Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma iÃ§in kullanÄ±lÄ±r
- Ã‡Ä±ktÄ±larÄ±n toplamÄ± 1'dir (olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±)
- Genellikle son katmanda kullanÄ±lÄ±r

## Aktivasyon Fonksiyonu SeÃ§imi

### Gizli Katmanlar Ä°Ã§in
- **ReLU**: VarsayÄ±lan seÃ§im, Ã§oÄŸu durumda iyi Ã§alÄ±ÅŸÄ±r
- **Leaky ReLU**: ReLU'nun Ã¶lÃ¼ nÃ¶ron problemi yaÅŸandÄ±ÄŸÄ±nda
- **Tanh**: SÄ±fÄ±r merkezli girdi gereken durumlarda

### Ã‡Ä±kÄ±ÅŸ KatmanÄ± Ä°Ã§in
- **Sigmoid**: Ä°kili sÄ±nÄ±flandÄ±rma
- **Softmax**: Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma
- **Linear**: Regresyon problemleri

## PyTorch ile Uygulama -> TensorFlow ile Uygulama

```python
# Model tanÄ±mlama
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Model derleme
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

## TensorFlow ile Ã–rnek Uygulama

```python
# Custom aktivasyon fonksiyonu
class CustomReLU(tf.keras.layers.Layer):
    def __init__(self, threshold=0.0):
        super(CustomReLU, self).__init__()
        self.threshold = threshold
    
    def call(self, inputs):
        return tf.maximum(self.threshold, inputs)

# Model oluÅŸturma
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64),
    CustomReLU(threshold=0.1),
    tf.keras.layers.Dense(32),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

## AlÄ±ÅŸtÄ±rmalar

1. FarklÄ± aktivasyon fonksiyonlarÄ±nÄ± numpy ile implement edin:
   - Sigmoid ve tÃ¼revi
   - ReLU ve tÃ¼revi
   - Tanh ve tÃ¼revi

2. MNIST veri seti Ã¼zerinde farklÄ± aktivasyon fonksiyonlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±n:
   - ReLU vs Tanh
   - Sigmoid vs Softmax (Ã§Ä±kÄ±ÅŸ katmanÄ±nda)
   - Leaky ReLU vs ReLU

3. Gradyan kaybÄ± problemini gÃ¶zlemleyin:
   - Derin bir aÄŸ oluÅŸturun
   - Sigmoid vs ReLU karÅŸÄ±laÅŸtÄ±rmasÄ± yapÄ±n
   - GradyanlarÄ± gÃ¶rselleÅŸtirin

## Kaynaklar
1. [CS231n - Neural Networks](http://cs231n.github.io/neural-networks-1/)
2. [Deep Learning Book - Activation Functions](https://www.deeplearningbook.org/contents/mlp.html)
3. [TensorFlow Activation Functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations) 