# Ã‡ok KatmanlÄ± Sinir AÄŸlarÄ±

## ğŸ“ BÃ¶lÃ¼m HaritasÄ±
- Ã–nceki BÃ¶lÃ¼m: [Perceptron](01-Perceptron.md)
- Sonraki BÃ¶lÃ¼m: [Aktivasyon Fonksiyonlari](03-Aktivasyon-Fonksiyonlari.md)
- Tahmini SÃ¼re: 4-5 saat
- Zorluk Seviyesi: ğŸŸ¡ Orta

## ğŸ¯ Hedefler
- Ã‡ok katmanlÄ± aÄŸ mimarisini anlama
- Ä°leri ve geri yayÄ±lÄ±mÄ± kavrama
- FarklÄ± aktivasyon fonksiyonlarÄ±nÄ± uygulama
- Model optimizasyonu yapabilme

## ğŸ¯ Ã–z DeÄŸerlendirme
- [ ] MLP mimarisini aÃ§Ä±klayabiliyorum
- [ ] Geri yayÄ±lÄ±m algoritmasÄ±nÄ± anlayabiliyorum
- [ ] Hiperparametre optimizasyonu yapabiliyorum
- [ ] FarklÄ± aktivasyon fonksiyonlarÄ±nÄ± kullanabiliyorum

## ğŸš€ Mini Projeler
1. MNIST SÄ±nÄ±flandÄ±rma
   - MLP modeli oluÅŸturma
   - Hiperparametre optimizasyonu
   - Model performans analizi

2. Regresyon Problemi
   - Boston Housing veri seti
   - FarklÄ± aktivasyon fonksiyonlarÄ±
   - Dropout implementasyonu

## ğŸ“‘ Ã–n KoÅŸullar

Ã‡ok KatmanlÄ± AÄŸlar (Multilayer Perceptron - MLP), tek katmanlÄ± perceptron'un sÄ±nÄ±rlamalarÄ±nÄ± aÅŸmak iÃ§in geliÅŸtirilmiÅŸ daha karmaÅŸÄ±k yapÄ±lardÄ±r.

![MLP YapÄ±sÄ±](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png)

## AÄŸ Mimarisi

### 1. GiriÅŸ KatmanÄ± (Input Layer)
```python
import tensorflow as tf

# GiriÅŸ katmanÄ±
input_layer = tf.keras.layers.Input(shape=(input_size,))
```

### 2. Gizli Katmanlar (Hidden Layers)
```python
# Gizli katmanlar
hidden_layer1 = tf.keras.layers.Dense(64, activation='relu')(input_layer)
hidden_layer2 = tf.keras.layers.Dense(32, activation='relu')(hidden_layer1)
```

### 3. Ã‡Ä±kÄ±ÅŸ KatmanÄ± (Output Layer)
```python
# SÄ±nÄ±flandÄ±rma iÃ§in
output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(hidden_layer2)

# Regresyon iÃ§in
output_layer = tf.keras.layers.Dense(1, activation='linear')(hidden_layer2)
```

## Ä°leri YayÄ±lÄ±m (Forward Propagation)

Her katmanda gerÃ§ekleÅŸen iÅŸlemler:
1. AÄŸÄ±rlÄ±klÄ± toplam
2. Aktivasyon fonksiyonu

```python
class MLPModel(tf.keras.Model):
    def __init__(self, layer_sizes):
        super(MLPModel, self).__init__()
        self.layers_list = []
        for size in layer_sizes[:-1]:
            self.layers_list.append(tf.keras.layers.Dense(size, activation='relu'))
        self.layers_list.append(tf.keras.layers.Dense(layer_sizes[-1]))
    
    def call(self, inputs):
        x = inputs
        for layer in self.layers_list:
            x = layer(x)
        return x
```

## Hiperparametreler

1. **Katman SayÄ±sÄ± ve Boyutu**
   - Ã‡ok az: Yetersiz Ã¶ÄŸrenme (underfitting)
   - Ã‡ok fazla: AÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting)

2. **Ã–ÄŸrenme OranÄ± (Learning Rate)**
   - Ã‡ok kÃ¼Ã§Ã¼k: YavaÅŸ Ã¶ÄŸrenme
   - Ã‡ok bÃ¼yÃ¼k: YakÄ±nsama sorunlarÄ±
```python
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
```

3. **Batch Size**
   - KÃ¼Ã§Ã¼k: Daha iyi genelleme
   - BÃ¼yÃ¼k: Daha hÄ±zlÄ± eÄŸitim
```python
model.fit(X_train, y_train, batch_size=32, epochs=100)
```

## Ã–rnek: XOR Problemi

```python
# Model oluÅŸturma
model = tf.keras.Sequential([
    tf.keras.layers.Dense(4, activation='relu', input_shape=(2,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# XOR verisi
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([0, 1, 1, 0])

# Model derleme
model.compile(optimizer='adam', 
             loss='binary_crossentropy',
             metrics=['accuracy'])

# Model eÄŸitimi
history = model.fit(X, y, epochs=1000, verbose=0)

# Test
predictions = model.predict(X)
print("XOR Ã‡Ä±ktÄ±larÄ±:")
for x_i, pred in zip(X, predictions):
    print(f"Girdi: {x_i}, Ã‡Ä±ktÄ±: {pred[0]:.2f}")
```

## DÃ¼zenlileÅŸtirme (Regularization)

### 1. Dropout
```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

### 2. L1/L2 Regularization
```python
regularizer = tf.keras.regularizers.l2(0.01)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, 
                         activation='relu',
                         kernel_regularizer=regularizer),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

## Model DeÄŸerlendirme ve Ä°zleme

```python
# Callback'ler
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'best_model.h5',
    monitor='val_loss',
    save_best_only=True
)

# Model eÄŸitimi
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    callbacks=[early_stopping, checkpoint]
)

# Ã–ÄŸrenme eÄŸrilerini Ã§izme
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

## AlÄ±ÅŸtÄ±rmalar

1. MNIST veri seti iÃ§in MLP modeli oluÅŸturun:
   - En az 2 gizli katman
   - Dropout kullanÄ±n
   - Model performansÄ±nÄ± deÄŸerlendirin

2. FarklÄ± hiperparametreleri deneyin:
   - Katman boyutlarÄ±
   - Ã–ÄŸrenme oranÄ±
   - Batch size

3. Overfitting ve underfitting durumlarÄ±nÄ± gÃ¶zlemleyin:
   - EÄŸitim/validasyon grafiklerini Ã§izin
   - DÃ¼zenlileÅŸtirme yÃ¶ntemlerini karÅŸÄ±laÅŸtÄ±rÄ±n

## Kaynaklar
1. [Deep Learning Book - Chapter 6](https://www.deeplearningbook.org/contents/mlp.html)
2. [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)
3. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) 