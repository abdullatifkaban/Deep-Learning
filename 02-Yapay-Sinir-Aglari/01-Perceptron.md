# Perceptron ve Yapay Sinir HÃ¼creleri

## ğŸ“ BÃ¶lÃ¼m HaritasÄ±
- Ã–nceki BÃ¶lÃ¼m: [Matematik Temelleri](../01-Temel-Kavramlar/03-Matematik-Temelleri.md)
- Sonraki BÃ¶lÃ¼m: [Ã‡ok KatmanlÄ± AÄŸlar](02-Cok-Katmanli-Aglar.md)
- Tahmini SÃ¼re: 3-4 saat
- Zorluk Seviyesi: ğŸŸ¢ BaÅŸlangÄ±Ã§

## ğŸ¯ Hedefler
- Perceptron yapÄ±sÄ±nÄ± ve Ã§alÄ±ÅŸma prensibini anlama
- Yapay sinir hÃ¼cresi bileÅŸenlerini Ã¶ÄŸrenme
- Temel Ã¶ÄŸrenme algoritmasÄ±nÄ± uygulama
- Perceptron'un sÄ±nÄ±rlamalarÄ±nÄ± kavrama

## ğŸ¯ Ã–z DeÄŸerlendirme
- [ ] Perceptron yapÄ±sÄ±nÄ± aÃ§Ä±klayabiliyorum
- [ ] Ã–ÄŸrenme algoritmasÄ±nÄ± uygulayabiliyorum
- [ ] Basit sÄ±nÄ±flandÄ±rma yapabiliyorum
- [ ] SÄ±nÄ±rlamalarÄ± anlayabiliyorum

## ğŸš€ Mini Projeler
1. MantÄ±k KapÄ±larÄ±
   - AND, OR kapÄ±larÄ± implementasyonu
   - XOR problemi analizi
   - SonuÃ§larÄ±n gÃ¶rselleÅŸtirilmesi

2. Ä°ris SÄ±nÄ±flandÄ±rma
   - Ä°kili sÄ±nÄ±flandÄ±rma
   - Performans deÄŸerlendirme
   - SÄ±nÄ±rlamalarÄ± gÃ¶zlemleme

## ğŸ“‘ Ã–n KoÅŸullar

## Biyolojik Sinir HÃ¼cresi

Yapay sinir aÄŸlarÄ±, biyolojik sinir sisteminden esinlenilerek geliÅŸtirilmiÅŸtir.

![Biyolojik NÃ¶ron](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/1200px-Neuron.svg.png)

### Biyolojik NÃ¶ronun BileÅŸenleri
1. **Dendritler**: DiÄŸer nÃ¶ronlardan gelen sinyalleri alÄ±r
2. **HÃ¼cre GÃ¶vdesi (Soma)**: Sinyalleri iÅŸler
3. **Akson**: Ä°ÅŸlenmiÅŸ sinyali diÄŸer nÃ¶ronlara iletir
4. **Sinaps**: NÃ¶ronlar arasÄ± baÄŸlantÄ± noktalarÄ±

## Yapay Sinir HÃ¼cresi (Perceptron)

Perceptron, yapay sinir aÄŸlarÄ±nÄ±n en temel yapÄ± taÅŸÄ±dÄ±r.

![Perceptron YapÄ±sÄ±](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Perceptron_moj.png/1200px-Perceptron_moj.png)

### Perceptron'un BileÅŸenleri
1. **Girdiler (xâ‚, xâ‚‚, ..., xâ‚™)**: NÃ¶ronun aldÄ±ÄŸÄ± veriler
2. **AÄŸÄ±rlÄ±klar (wâ‚, wâ‚‚, ..., wâ‚™)**: Her girdinin Ã¶nem derecesi
3. **Bias (b)**: EÅŸik deÄŸeri
4. **Toplama Fonksiyonu**: AÄŸÄ±rlÄ±klÄ± toplamÄ± hesaplar
5. **Aktivasyon Fonksiyonu**: Ã‡Ä±ktÄ±yÄ± belirler

### Matematiksel Model
```python
import tensorflow as tf
import numpy as np

class Perceptron(tf.keras.Model):
    def __init__(self, input_size):
        super(Perceptron, self).__init__()
        self.dense = tf.keras.layers.Dense(1, use_bias=True)
        
    def call(self, x):
        # AÄŸÄ±rlÄ±klÄ± toplam ve bias
        z = self.dense(x)
        # Step fonksiyonu
        return tf.cast(tf.greater(z, 0), tf.float32)
```

## Ã–ÄŸrenme SÃ¼reci

### 1. Ä°leri YayÄ±lÄ±m (Forward Propagation)
```python
@tf.function
def predict(self, x):
    return self.call(x)
```

### 2. Hata Hesaplama
```python
@tf.function
def calculate_error(y_true, y_pred):
    return tf.cast(y_true, tf.float32) - y_pred
```

### 3. AÄŸÄ±rlÄ±k GÃ¼ncelleme
```python
@tf.function
def train_step(self, x, y, learning_rate=0.1):
    with tf.GradientTape() as tape:
        predictions = self.call(x)
        error = self.calculate_error(y, predictions)
    
    gradients = tape.gradient(error, self.trainable_variables)
    for grad, var in zip(gradients, self.trainable_variables):
        var.assign_add(learning_rate * grad)
```

## Ã–rnek: AND KapÄ±sÄ± UygulamasÄ±

```python
# EÄŸitim verisi
X = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)
y = np.array([0, 0, 0, 1], dtype=np.float32)

# Model oluÅŸturma
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, activation='step', input_shape=(2,))
])

# Model derleme
model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),
             loss='binary_crossentropy',
             metrics=['accuracy'])

# Model eÄŸitimi
history = model.fit(X, y, epochs=100, verbose=0)

# Test
predictions = model.predict(X)
for x_i, pred in zip(X, predictions):
    print(f"Girdi: {x_i}, Ã‡Ä±ktÄ±: {int(pred[0])}")
```

## Perceptron'un SÄ±nÄ±rlamalarÄ±

1. **DoÄŸrusal AyrÄ±labilirlik**
   - Sadece doÄŸrusal olarak ayrÄ±labilen problemleri Ã§Ã¶zebilir
   - XOR problemi gibi doÄŸrusal olmayan problemleri Ã§Ã¶zemez

![DoÄŸrusal AyrÄ±labilirlik](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Linearly_separable_data.png/1200px-Linearly_separable_data.png)

2. **Tek Katman**
   - KarmaÅŸÄ±k Ã¶rÃ¼ntÃ¼leri Ã¶ÄŸrenemez
   - Ã‡ok katmanlÄ± problemler iÃ§in yetersiz

## Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±ya GeÃ§iÅŸ

Perceptron'un sÄ±nÄ±rlamalarÄ±nÄ± aÅŸmak iÃ§in:
1. Ã‡ok katmanlÄ± yapÄ±
2. FarklÄ± aktivasyon fonksiyonlarÄ±
3. Geri yayÄ±lÄ±m algoritmasÄ±

## AlÄ±ÅŸtÄ±rmalar

### BaÅŸlangÄ±Ã§ Seviyesi
1. AND ve OR kapÄ±larÄ± implementasyonu
2. Basit ikili sÄ±nÄ±flandÄ±rma

### Orta Seviye
1. XOR problemi analizi
2. Ä°ris veri seti sÄ±nÄ±flandÄ±rma

### Ä°leri Seviye
1. Ã‡ok sÄ±nÄ±flÄ± perceptron
2. FarklÄ± aktivasyon fonksiyonlarÄ±

## ğŸ“š Ã–nerilen Kaynaklar
1. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
2. [Deep Learning Book - Chapter 6](https://www.deeplearningbook.org/contents/mlp.html)
3. [TensorFlow Documentation](https://www.tensorflow.org/guide/keras/custom_layers_and_models) 