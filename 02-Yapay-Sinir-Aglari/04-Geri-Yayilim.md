# Geri YayÄ±lÄ±m AlgoritmasÄ±

## ğŸ“ BÃ¶lÃ¼m HaritasÄ±
- Ã–nceki BÃ¶lÃ¼m: [Aktivasyon FonksiyonlarÄ±](03-Aktivasyon-Fonksiyonlari.md)
- Sonraki BÃ¶lÃ¼m: [CNN Mimarisi](../03-Derin-Ogrenme-Modelleri/01-CNN.md)
- Tahmini SÃ¼re: 4-5 saat
- Zorluk Seviyesi: ğŸŸ¡ Orta

## ğŸ¯ Hedefler
- Geri yayÄ±lÄ±m algoritmasÄ±nÄ±n mantÄ±ÄŸÄ±nÄ± anlama
- Gradyan hesaplamalarÄ±nÄ± kavrama
- Zincir kuralÄ±nÄ± uygulama
- Optimizasyon tekniklerini Ã¶ÄŸrenme

## ğŸ¯ Ã–z DeÄŸerlendirme
- [ ] Geri yayÄ±lÄ±m algoritmasÄ±nÄ± aÃ§Ä±klayabiliyorum
- [ ] Gradyan hesaplamalarÄ±nÄ± yapabiliyorum
- [ ] Zincir kuralÄ±nÄ± uygulayabiliyorum
- [ ] FarklÄ± optimizasyon tekniklerini kullanabiliyorum

## ğŸš€ Mini Projeler
1. Manuel Geri YayÄ±lÄ±m
   - Basit sinir aÄŸÄ± implementasyonu
   - Gradyan hesaplamalarÄ±
   - AÄŸÄ±rlÄ±k gÃ¼ncellemeleri

2. Optimizasyon KarÅŸÄ±laÅŸtÄ±rmasÄ±
   - SGD vs Adam
   - Momentum implementasyonu
   - Ã–ÄŸrenme oranÄ± planlamasÄ±

## ğŸ“‘ Ã–n KoÅŸullar
- TÃ¼rev ve zincir kuralÄ± bilgisi
- Python ve NumPy deneyimi
- Temel lineer cebir
- Optimizasyon kavramlarÄ±

## ğŸ”‘ Temel Kavramlar
1. Gradyan Ä°niÅŸ
2. Zincir KuralÄ±
3. Hata FonksiyonlarÄ±
4. Optimizasyon Teknikleri

## GiriÅŸ

Geri yayÄ±lÄ±m algoritmasÄ±, yapay sinir aÄŸlarÄ±nÄ±n eÄŸitiminde kullanÄ±lan temel optimizasyon yÃ¶ntemidir. Bu algoritma, aÄŸÄ±n Ã§Ä±ktÄ±sÄ± ile beklenen Ã§Ä±ktÄ± arasÄ±ndaki hatayÄ± geriye doÄŸru yayarak aÄŸÄ±rlÄ±klarÄ± gÃ¼nceller.

![Backpropagation](https://upload.wikimedia.org/wikipedia/commons/4/4f/Backpropagation_Algorithm.gif)

## AlgoritmanÄ±n AdÄ±mlarÄ±

### 1. Ä°leri YayÄ±lÄ±m (Forward Propagation)
```python
import tensorflow as tf

class SimpleNN(tf.keras.Model):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.dense1 = tf.keras.layers.Dense(64, activation='relu')
        self.dense2 = tf.keras.layers.Dense(32, activation='relu')
        self.dense3 = tf.keras.layers.Dense(10, activation='softmax')
    
    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        return self.dense3(x)
```

### 2. Hata Hesaplama
```python
# Kategorik Ã§apraz entropi kaybÄ±
loss_fn = tf.keras.losses.CategoricalCrossentropy()

# MSE kaybÄ±
loss_fn = tf.keras.losses.MeanSquaredError()
```

### 3. Gradyan Hesaplama ve GÃ¼ncelleme
```python
@tf.function
def train_step(model, inputs, targets, optimizer):
    with tf.GradientTape() as tape:
        predictions = model(inputs, training=True)
        loss = loss_fn(targets, predictions)
    
    # GradyanlarÄ± hesapla
    gradients = tape.gradient(loss, model.trainable_variables)
    
    # AÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    return loss
```

## Tam Implementasyon

```python
class NeuralNetwork(tf.keras.Model):
    def __init__(self, layer_sizes):
        super(NeuralNetwork, self).__init__()
        self.layers_list = []
        
        for size in layer_sizes[:-1]:
            self.layers_list.append(tf.keras.layers.Dense(size, activation='relu'))
        self.layers_list.append(tf.keras.layers.Dense(layer_sizes[-1]))
    
    def call(self, inputs):
        x = inputs
        for layer in self.layers_list:
            x = layer(x)
        return x

# Model eÄŸitimi
@tf.function
def train_model(model, dataset, optimizer, epochs):
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in dataset:
            loss = train_step(model, batch_x, batch_y, optimizer)
            total_loss += loss
            
        print(f"Epoch {epoch+1}, Loss: {total_loss/len(dataset):.4f}")
```

## Optimizasyon Teknikleri

### 1. Momentum
```python
optimizer = tf.keras.optimizers.SGD(
    learning_rate=0.01,
    momentum=0.9
)
```

### 2. Adam Optimizer
```python
optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07
)
```

## Ã–rnek: MNIST SÄ±nÄ±flandÄ±rma

```python
# Veri yÃ¼kleme
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Veri Ã¶n iÅŸleme
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# Dataset oluÅŸturma
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)

# Model oluÅŸturma
model = NeuralNetwork([784, 128, 64, 10])

# Optimizer ve loss fonksiyonu
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# Model derleme
model.compile(optimizer=optimizer,
             loss=loss_fn,
             metrics=['accuracy'])

# Model eÄŸitimi
history = model.fit(train_dataset,
                   epochs=10,
                   validation_data=(x_test, y_test))
```

## Gradyan GÃ¶rselleÅŸtirme

```python
# GradyanlarÄ± izleme
@tf.function
def get_gradients(model, inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = loss_fn(targets, predictions)
    return tape.gradient(loss, model.trainable_variables)

# GradyanlarÄ± gÃ¶rselleÅŸtirme
def plot_gradients(gradients):
    plt.figure(figsize=(10, 6))
    for i, grad in enumerate(gradients):
        plt.subplot(len(gradients), 1, i+1)
        plt.hist(grad.numpy().flatten(), bins=50)
        plt.title(f'Layer {i+1} Gradients')
    plt.tight_layout()
    plt.show()
```

## Vanishing ve Exploding Gradients

### Vanishing Gradients
- Derin aÄŸlarda gradyanlarÄ±n Ã§ok kÃ¼Ã§Ã¼lmesi
- Ã‡Ã¶zÃ¼m: ReLU aktivasyonu, LSTM, ResNet

### Exploding Gradients
- GradyanlarÄ±n Ã§ok bÃ¼yÃ¼mesi
- Ã‡Ã¶zÃ¼m: Gradient Clipping, Weight Initialization

## AlÄ±ÅŸtÄ±rmalar

1. XOR problemi iÃ§in:
   - 2-2-1 mimarisinde bir aÄŸ oluÅŸturun
   - Geri yayÄ±lÄ±m algoritmasÄ±nÄ± implement edin
   - FarklÄ± Ã¶ÄŸrenme oranlarÄ±nÄ± deneyin

2. MNIST veri seti iÃ§in:
   - Ã‡ok katmanlÄ± bir aÄŸ oluÅŸturun
   - Momentum ve Adam optimizerlarÄ± karÅŸÄ±laÅŸtÄ±rÄ±n
   - Ã–ÄŸrenme eÄŸrilerini Ã§izin

3. Vanishing Gradient problemi:
   - Derin bir aÄŸ oluÅŸturun
   - Sigmoid ve ReLU aktivasyonlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±n
   - GradyanlarÄ± gÃ¶rselleÅŸtirin

## Kaynaklar
1. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html)
2. [TensorFlow Tutorial - Custom Training](https://www.tensorflow.org/tutorials/customization/custom_training)
3. [Deep Learning Book - Optimization](https://www.deeplearningbook.org/contents/optimization.html) 